{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"provenance":[],"gpuType":"V28"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","source":["Ref) https://github.com/adityashukzy/Vanilla-GAN/blob/main/VanillaGAN.ipynb"],"metadata":{"id":"r_EWPJWS3tbA"}},{"cell_type":"markdown","source":["###Inquiery)\n","Goodfellow presented GAN in 2014. I want to get a python source code which is most similar to the original implementation of Goodfellow's. When I searched over internet, I could find only the version of Radford's (2016 ICLR), which is built on deep convolution layers. Can you help me find Goodfellow's version. I would be happy if the version can be run on Tensorflow 2.0 or later.\n","\n","ChatGPT의 말:\n","###ChatGPT)\n","Ian Goodfellow's original 2014 Generative Adversarial Network (GAN) utilized fully connected neural networks (multilayer perceptrons) for both the generator and discriminator models. While the original implementation was in Theano, there are Python implementations that closely replicate this architecture using TensorFlow 2.x.\n","\n","One such implementation is available on GitHub:   \n","Vanilla-GAN: This repository offers a straightforward PyTorch implementation of Goodfellow's original GAN architecture applied to the MNIST dataset.\n","###Ref) https://github.com/adityashukzy/Vanilla-GAN/blob/main/VanillaGAN.ipynb\n","To adapt this PyTorch code to TensorFlow 2.x, you can follow these steps:\n","\n","Define the Generator and Discriminator Models: Use TensorFlow's tf.keras.Sequential to create the models with fully connected layers, mirroring the original architecture.\n","\n","Implement the Training Loop: Incorporate the adversarial training process, ensuring the generator and discriminator are updated appropriately.\n","\n","Utilize TensorFlow's Optimizers and Loss Functions: Leverage TensorFlow's built-in functions to handle the optimization and loss calculations.\n","\n","By following these steps, you can recreate Goodfellow's original GAN using TensorFlow 2.x, enabling you to experiment with and understand the foundational GAN architecture."],"metadata":{"id":"M6Aqckkg389T"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QxgCeexlN-Kr","outputId":"7fc03e3b-c40a-4d1f-e6f2-7d853948a0e4","executionInfo":{"status":"ok","timestamp":1731992273855,"user_tz":-540,"elapsed":2529,"user":{"displayName":"Yong Cheol Kim","userId":"03608596061589998053"}}},"source":["!pip install torchvision tensorboardx matplotlib numpy"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cpu)\n","Collecting tensorboardx\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n","Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.1+cpu)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardx) (24.2)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardx) (4.25.5)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n","Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tensorboardx\n","Successfully installed tensorboardx-2.6.2.2\n"]}]},{"cell_type":"code","metadata":{"id":"ko7iW4b3OFfO","executionInfo":{"status":"ok","timestamp":1731992288204,"user_tz":-540,"elapsed":10905,"user":{"displayName":"Yong Cheol Kim","userId":"03608596061589998053"}}},"source":["import torch\n","from torch import nn, optim\n","from torch import Tensor\n","from torchvision import transforms, datasets\n","import matplotlib.pyplot as plt"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Jh_kuhYOYzE","executionInfo":{"status":"ok","timestamp":1731992290583,"user_tz":-540,"elapsed":368,"user":{"displayName":"Yong Cheol Kim","userId":"03608596061589998053"}}},"source":["def preprocessData():\n","    trans = transforms.Compose([\n","        transforms.ToTensor(), # converts image of (1, 255) to (0, 1)\n","        transforms.Normalize((.5,), (.5,)) # normalizes (0, 1) to (-1, 1)\n","    ])\n","    dataset_dir = '/content/MNISTdataset'\n","    return datasets.MNIST(root = dataset_dir, train = True, transform = trans, download = True)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vip52vBwSyFj","outputId":"96e87be5-d699-495e-8018-d1b90edf044c","executionInfo":{"status":"ok","timestamp":1731992297938,"user_tz":-540,"elapsed":3003,"user":{"displayName":"Yong Cheol Kim","userId":"03608596061589998053"}}},"source":["data = preprocessData()\n","\n","dataLoader = torch.utils.data.DataLoader(data, batch_size = 100, shuffle = True)\n","noOfBatches = len(dataLoader)\n","print(\"Number of batches: {}\".format(noOfBatches))"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /content/MNISTdataset/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 33.1MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting /content/MNISTdataset/MNIST/raw/train-images-idx3-ubyte.gz to /content/MNISTdataset/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /content/MNISTdataset/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28.9k/28.9k [00:00<00:00, 778kB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting /content/MNISTdataset/MNIST/raw/train-labels-idx1-ubyte.gz to /content/MNISTdataset/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /content/MNISTdataset/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1.65M/1.65M [00:00<00:00, 10.6MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting /content/MNISTdataset/MNIST/raw/t10k-images-idx3-ubyte.gz to /content/MNISTdataset/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /content/MNISTdataset/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4.54k/4.54k [00:00<00:00, 6.87MB/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting /content/MNISTdataset/MNIST/raw/t10k-labels-idx1-ubyte.gz to /content/MNISTdataset/MNIST/raw\n","\n","Number of batches: 600\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","metadata":{"id":"k_M1HEUIT2a2","executionInfo":{"status":"ok","timestamp":1731992309494,"user_tz":-540,"elapsed":389,"user":{"displayName":"Yong Cheol Kim","userId":"03608596061589998053"}}},"source":["class Discriminator(torch.nn.Module):\n","    '''\n","    3 hidden-layer Discriminator Neural Network\n","    '''\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        n_features = 784 # 28 x 28 image flattened to a (784, 1) vec\n","        n_out = 1\n","\n","        self.hidden0 = nn.Sequential(\n","            nn.Linear(n_features, 1024),\n","            nn.LeakyReLU(),\n","            nn.Dropout(0.3)\n","        )\n","\n","        self.hidden1 = nn.Sequential(\n","            nn.Linear(1024, 512),\n","            nn.LeakyReLU(),\n","            nn.Dropout(0.3)\n","        )\n","\n","        self.hidden2 = nn.Sequential(\n","            nn.Linear(512, 256),\n","            nn.LeakyReLU(),\n","            nn.Dropout(0.3)\n","        )\n","\n","        self.outLayer = nn.Sequential(\n","            nn.Linear(256, n_out),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, inp):\n","        x = self.hidden0(inp)\n","        x = self.hidden1(x)\n","        x = self.hidden2(x)\n","        output = self.outLayer(x)\n","\n","        return output"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"5R6rVAtJ8UKN","executionInfo":{"status":"ok","timestamp":1731992313875,"user_tz":-540,"elapsed":373,"user":{"displayName":"Yong Cheol Kim","userId":"03608596061589998053"}}},"source":["class Generator(torch.nn.Module):\n","    '''\n","    3-layer Generative Neural Network\n","    '''\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        n_features = 100 # random noise of size (100, 1)\n","        n_out = 784 # output image of (784, 1) which will be resized to (28, 28)\n","\n","        self.hidden0 = nn.Sequential(\n","            nn.Linear(n_features, 256),\n","            nn.LeakyReLU()\n","        )\n","\n","        self.hidden1 = nn.Sequential(\n","            nn.Linear(256, 512),\n","            nn.LeakyReLU()\n","        )\n","\n","        self.hidden2 = nn.Sequential(\n","            nn.Linear(512, 1024),\n","            nn.LeakyReLU()\n","        )\n","\n","        self.outLayer = nn.Sequential(\n","            nn.Linear(1024, n_out),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, inp):\n","        x = self.hidden0(inp)\n","        x = self.hidden1(x)\n","        x = self.hidden2(x)\n","        output = self.outLayer(x)\n","\n","        return output"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"blM_d2qVIYeY","executionInfo":{"status":"ok","timestamp":1731992318356,"user_tz":-540,"elapsed":360,"user":{"displayName":"Yong Cheol Kim","userId":"03608596061589998053"}}},"source":["def images_to_vectors(images):\n","    return images.view(images.size(0), 784)\n","\n","def vectors_to_images(vectors):\n","    '''\n","    Resizes a flattened vector into (1, 28, 28)\n","    '''\n","    return vectors.view(vectors.size(0), 28, 28)\n","\n","def noise_generator(size):\n","    '''\n","    torch.randn() generates random Gaussian noise with mean = 0 and variance = 1 i.e. a standard normal distribution\n","\n","    Implementational Note ->\n","    the use of Variable here is a bit weird to me. In general, Tensors and Variables in PyTorch are the same thing;\n","    Variables have the added functionality of constructing a chain (or dynamic graph) of computation so that it can backprop\n","    through it. We can use this easily if we just do requires_grad = True. Here, obviously we're not doing that so I think I'll\n","    experiment with just using Tensor later to see if it still works.\n","\n","    UPDATE: Variable has been deprecated. All its functionality now just works with Tensor instead. Just need to set\n","    requires_grad to True.\n","    '''\n","\n","    noise = Tensor(torch.randn(size, 100))\n","    return noise"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"RfZRCGOLrLc4","executionInfo":{"status":"ok","timestamp":1731995427784,"user_tz":-540,"elapsed":362,"user":{"displayName":"Yong Cheol Kim","userId":"03608596061589998053"}}},"source":["# instantiating Discriminator and Generator models\n","discriminator = Discriminator()\n","generator = Generator()\n","\n","learning_rate = 0.0002\n","\n","# Adam is reknown as perhaps one of the best optimizers for GANs\n","discriminatorOptimizer = optim.Adam(discriminator.parameters(), lr=learning_rate)\n","generatorOptimizer = optim.Adam(generator.parameters(), lr=learning_rate)\n","\n","# Binary Cross-entropy Loss\n","# since it closely models the loss function we use here\n","loss = nn.BCELoss()"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"2jhRMy7ivMat","executionInfo":{"status":"ok","timestamp":1731995431286,"user_tz":-540,"elapsed":365,"user":{"displayName":"Yong Cheol Kim","userId":"03608596061589998053"}}},"source":["def ones_target(size):\n","    '''\n","    Returns a tensor of ones of (size, 1) which will act as the label for the real data\n","\n","    Params:\n","    size => describes the number of training examples in the minibatch so that the label we return = (m, 1) for all m real examples (of the minibatch)\n","            in short: [l0, l1, l2, ....., lm] which here => [1, 1, 1, .... , 1]\n","    '''\n","    return Tensor(torch.ones(size, 1))\n","\n","def zeros_target(size):\n","    '''\n","    Returns a tensor of zeros of (size, 1) which will act as the label for the fake data\n","\n","    Params:\n","    size => describes the number of training examples in the minibatch so that the label we return = (m, 1) for all m fake examples (of the minibatch)\n","    in short: [l0, l1, l2, ....., lm] which here => [0, 0, 0, .... , 0]\n","    '''\n","    return Tensor(torch.zeros(size, 1))"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["## fakeError = loss(fakePreds, zeros_target(size))\n","\n","Gemini\n","Okay, let's break down the line of code loss(fakePreds, zeros_target(size)):\n","\n","This line calculates the error or loss of the discriminator when it's trying to classify fake images. Here's a step-by-step explanation:\n","\n","- **loss**: This is a pre-defined variable representing the loss function used to train the discriminator. In this case, it's **nn.BCELoss() which stands for Binary Cross-Entropy Loss**. A loss function, in simple terms, quantifies how well the model's predictions match the actual values. The lower the loss, the better the model's performance.   \n","\n","#### 즉, DCGAN에서의 loss와 비슷한 방법이다.\n","- You can imagine the loss function like a penalty system. The more the discriminator's predictions are wrong, the higher the penalty or loss it receives.\n","fakePreds: This variable stores the discriminator's predictions on a batch of fake images generated by the generator. These predictions are essentially probabilities that the discriminator assigns to each image, indicating how likely it thinks the image is real.\n","\n","- **zeros_target(size)**: This function call generates a tensor of zeros with a shape determined by size. This tensor represents the target labels for the fake images. We're using zeros as the target because we want the discriminator to **classify fake images as fake (represented by 0)**.\n","\n","size refers to the number of fake images in the current batch.\n","Putting it all together:\n","\n","- **loss(fakePreds, zeros_target(size))** calculates the Binary Cross-Entropy Loss between the discriminator's predictions on fake images (fakePreds) and the desired target labels (zeros_target(size)).\n","\n","This calculation essentially measures how well the discriminator is doing at correctly identifying fake images. The discriminator is penalized for assigning high probabilities (close to 1) to fake images. The larger the difference between the predictions and the target labels, the higher the loss value will be.\n","\n","- **In summary**, this line of code is a crucial step in training the discriminator to distinguish between real and fake images. It quantifies the discriminator's error on fake images, providing a signal for the optimization process to adjust the discriminator's parameters and improve its performance."],"metadata":{"id":"dDUkFooM2y_S"}},{"cell_type":"code","metadata":{"id":"f8miLR0qvqF-","executionInfo":{"status":"ok","timestamp":1731995434171,"user_tz":-540,"elapsed":357,"user":{"displayName":"Yong Cheol Kim","userId":"03608596061589998053"}}},"source":["def trainDiscriminator(optimizer, fakeData, realData):\n","    size = realData.size(0) # basically getting the number of training examples in the minibatch so it can construct a vector of (m, 1) labels later on\n","    # in short, it is the minibatch size\n","\n","    # PyTorch accumulates gradient values on each loss.backward(), so we reset the gradients to zero at each iteration to do parameter update correctly\n","    optimizer.zero_grad()\n","\n","    '''\n","    fakePreds.shape= torch.Size([100, 1])\n","    zeros_target(size).shape= torch.Size([100, 1])\n","    realPreds.shape= torch.Size([100, 1])\n","    '''\n","\n","    # Training on Fake Data\n","    fakePreds = discriminator(fakeData)\n","    #print('fakePreds.shape=', fakePreds.shape)\n","    #print('zeros_target(size).shape=', zeros_target(size).shape)\n","    # 위의 cell의 설명을 읽으시오,\n","    fakeError = loss(fakePreds, zeros_target(size))\n","    fakeError.backward() # calculating gradients going back\n","\n","    # Training on Real Data\n","    realPreds = discriminator(realData)\n","    #print('realPreds.shape=', realPreds.shape)\n","    #print(ones_target(size).shape)\n","    realError = loss(realPreds, ones_target(size))\n","    realError.backward() # calculating gradients going back\n","\n","    # Finally updating the parameters once we have made a forward pass and calculated grads\n","    optimizer.step()\n","\n","    return realError + fakeError, realPreds, fakePreds"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"yHv3mZlnyaFY","executionInfo":{"status":"ok","timestamp":1731995436217,"user_tz":-540,"elapsed":354,"user":{"displayName":"Yong Cheol Kim","userId":"03608596061589998053"}}},"source":["def trainGenerator(optimizer, fakeData):\n","    size = fakeData.size(0) # again, minibatch size\n","\n","    optimizer.zero_grad()\n","\n","    predictions = discriminator(fakeData)\n","    error = loss(predictions, ones_target(size)) # cos we want our data to be classified as real, ie [1, 1, 1, ...., 1] for each sample in the minibatch\n","    error.backward()\n","\n","    optimizer.step()\n","\n","    return error"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"XevIaKgGTDvL","executionInfo":{"status":"ok","timestamp":1731995437463,"user_tz":-540,"elapsed":1,"user":{"displayName":"Yong Cheol Kim","userId":"03608596061589998053"}}},"source":["testSamples = 16\n","testNoise = noise_generator(testSamples)"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1b0pAkQnNc3tp8dMdxG93cS1ZSa0L_whv"},"id":"QRBSiErD1Dds","outputId":"a5035bdd-6bad-4b59-bf1e-50b573414783","executionInfo":{"status":"ok","timestamp":1731998867111,"user_tz":-540,"elapsed":3428417,"user":{"displayName":"Yong Cheol Kim","userId":"03608596061589998053"}}},"source":["numEpochs = 100\n","#numEpochs = 3\n","\n","for epoch in range(numEpochs):\n","    print(\"epoch=\",epoch)\n","\n","    for nBatch, (realData, _) in enumerate(dataLoader): # enumerate obv. returns tuple of (batch_number, dataLoader)\n","                                                        # while dataLoader returns (data, targetLabel)\n","        size = realData.size(0)\n","\n","        # Train the discriminator\n","        realData = images_to_vectors(realData) # prepping real data\n","        fakeData = generator(noise_generator(size)).detach() # generating fake data\n","        # .detach() ensures grads are not calculated for generator here since we're not training it as yet\n","\n","        discError, discRealPreds, discFakePreds = trainDiscriminator(discriminatorOptimizer, fakeData, realData)\n","\n","        # Train the generator\n","        moreFakeData = generator(noise_generator(size)) # generating fake data\n","\n","        genError = trainGenerator(generatorOptimizer, moreFakeData)\n","\n","        #if nBatch % 100 == 0:\n","        if nBatch % 1000 == 0:\n","            testImages = vectors_to_images(generator(testNoise))\n","            #print(\"New Batch of Images: => nBatch=\",nBatch)\n","\n","            axes = []\n","            fig = plt.figure(figsize=(20, 4))\n","            for i in range(testImages.shape[0]):\n","                axes.append(fig.add_subplot(2, 8, i + 1))\n","                plt.imshow(testImages[i].detach().numpy())\n","                axes[-1].set_title(\"Image {}\".format(i + 1))\n","                plt.axis('off')\n","                i += 1\n","            plt.show()"],"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"03dTLGBoTKF3"},"source":["print(\"Voila!\")"],"execution_count":null,"outputs":[]}]}